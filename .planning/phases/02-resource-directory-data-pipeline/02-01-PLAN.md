---
phase: 02-resource-directory-data-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/00002_directory_schema.sql
  - src/lib/db/types.ts
  - src/lib/etl/validators/organization.ts
  - src/lib/etl/parsers/organization-import.ts
  - scripts/import-organizations.ts
autonomous: true

must_haves:
  truths:
    - "organizations table exists in Supabase with tsvector FTS column and GIN index"
    - "businesses table exists in Supabase with tsvector FTS column and GIN index"
    - "85K+ organization records are imported from CSV into organizations table"
    - "Invalid CSV rows are rejected and logged (not silently dropped)"
    - "Each organization has last_verified_date, confidence_score, and confidence_grade"
  artifacts:
    - path: "supabase/migrations/00002_directory_schema.sql"
      provides: "organizations + businesses tables, FTS indexes, freshness function, RLS policies"
      contains: "CREATE TABLE public.organizations"
    - path: "src/lib/db/types.ts"
      provides: "TypeScript types for Organization and Business records"
      exports: ["Organization", "Business", "FreshnessStatus"]
    - path: "src/lib/etl/validators/organization.ts"
      provides: "Zod schema for validating CSV organization rows"
      exports: ["OrganizationSchema", "validateOrganizationRow"]
    - path: "src/lib/etl/parsers/organization-import.ts"
      provides: "Streaming CSV parser with batch insert for organizations"
      exports: ["importOrganizations"]
    - path: "scripts/import-organizations.ts"
      provides: "CLI entry point that runs the organization import"
  key_links:
    - from: "src/lib/etl/parsers/organization-import.ts"
      to: "src/lib/etl/validators/organization.ts"
      via: "Zod validation before insert"
      pattern: "OrganizationSchema\\.safeParse"
    - from: "src/lib/etl/parsers/organization-import.ts"
      to: "supabase organizations table"
      via: "Supabase client batch insert"
      pattern: "supabase\\.from\\('organizations'\\)\\.insert"
    - from: "supabase/migrations/00002_directory_schema.sql"
      to: "src/lib/db/types.ts"
      via: "TypeScript types mirror database columns"
---

<objective>
Create the database schema for the resource directory (organizations + businesses tables with full-text search, freshness tracking, and RLS) and build the ETL pipeline that imports 85K+ veteran organizations from the vet_org_directory CSV.

Purpose: The database schema is the foundation for all directory features. The organization ETL import populates the primary dataset that veterans will search. Without data, no search UI can function.

Output: Migration SQL creating both directory tables, TypeScript types, Zod validation schemas, streaming CSV import pipeline, and 85K+ organization records in the database.
</objective>

<execution_context>
@/Users/vincecain/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vincecain/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-resource-directory-data-pipeline/02-RESEARCH.md
@.planning/phases/01-foundation-crisis-safety/01-02-SUMMARY.md
@supabase/migrations/00001_initial_schema.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create directory database schema with FTS and freshness tracking</name>
  <files>
    supabase/migrations/00002_directory_schema.sql
    src/lib/db/types.ts
  </files>
  <action>
Create migration `supabase/migrations/00002_directory_schema.sql` with:

**organizations table:**
- id UUID PRIMARY KEY DEFAULT gen_random_uuid()
- ein TEXT (unique identifier from IRS, nullable for non-IRS sources)
- org_name TEXT NOT NULL
- org_name_alt TEXT
- org_type TEXT (e.g. 501c3, 501c19)
- street_address TEXT, street_address_2 TEXT, city TEXT, state TEXT (2-letter), zip_code TEXT, country TEXT DEFAULT 'US'
- phone TEXT, email TEXT, website TEXT
- ntee_code TEXT, ntee_description TEXT
- irs_subsection TEXT, tax_exempt_status TEXT
- mission_statement TEXT, services_offered TEXT
- service_categories TEXT (semicolon-separated list)
- eligibility_requirements TEXT, service_area TEXT
- total_revenue NUMERIC, total_expenses NUMERIC, total_assets NUMERIC
- num_employees INTEGER, num_volunteers INTEGER
- charity_navigator_rating NUMERIC(3,1), charity_navigator_score NUMERIC(5,1)
- va_accredited BOOLEAN DEFAULT false
- data_sources TEXT[] (PostgreSQL array)
- last_verified_date DATE NOT NULL DEFAULT CURRENT_DATE
- verification_method TEXT DEFAULT 'csv_import'
- confidence_score NUMERIC(3,2) CHECK (confidence_score BETWEEN 0 AND 1)
- confidence_grade TEXT CHECK (confidence_grade IN ('A', 'B', 'C', 'D', 'F'))
- created_at TIMESTAMPTZ NOT NULL DEFAULT now()
- updated_at TIMESTAMPTZ NOT NULL DEFAULT now()

Add generated tsvector column for full-text search:
```sql
fts tsvector GENERATED ALWAYS AS (
  setweight(to_tsvector('english', COALESCE(org_name, '')), 'A') ||
  setweight(to_tsvector('english', COALESCE(mission_statement, '')), 'B') ||
  setweight(to_tsvector('english', COALESCE(services_offered, '')), 'B') ||
  setweight(to_tsvector('english', COALESCE(service_categories, '')), 'C') ||
  setweight(to_tsvector('english', COALESCE(ntee_description, '')), 'C')
) STORED
```

Create GIN index: `CREATE INDEX organizations_fts_idx ON organizations USING GIN (fts);`
Create index on state: `CREATE INDEX organizations_state_idx ON organizations(state);`
Create index on confidence_grade: `CREATE INDEX organizations_grade_idx ON organizations(confidence_grade);`
Create unique partial index on ein: `CREATE UNIQUE INDEX organizations_ein_idx ON organizations(ein) WHERE ein IS NOT NULL;`

**businesses table:**
- id UUID PRIMARY KEY DEFAULT gen_random_uuid()
- uei TEXT (unique entity identifier from SAM.gov)
- cage_code TEXT
- legal_business_name TEXT NOT NULL
- dba_name TEXT
- physical_address_line1 TEXT, physical_address_line2 TEXT, city TEXT, state TEXT, zip_code TEXT, country TEXT DEFAULT 'USA'
- phone TEXT, email TEXT, website TEXT
- business_type TEXT (e.g. 'Veteran Owned Small Business', 'Service Disabled Veteran Owned Small Business')
- naics_codes TEXT, naics_descriptions TEXT
- owner_name TEXT, service_branch TEXT
- registration_status TEXT, registration_expiration TEXT
- latitude NUMERIC, longitude NUMERIC
- source TEXT
- last_verified_date DATE NOT NULL DEFAULT CURRENT_DATE
- verification_method TEXT DEFAULT 'csv_import'
- created_at TIMESTAMPTZ NOT NULL DEFAULT now()
- updated_at TIMESTAMPTZ NOT NULL DEFAULT now()

Add generated tsvector column for businesses:
```sql
fts tsvector GENERATED ALWAYS AS (
  setweight(to_tsvector('english', COALESCE(legal_business_name, '')), 'A') ||
  setweight(to_tsvector('english', COALESCE(dba_name, '')), 'A') ||
  setweight(to_tsvector('english', COALESCE(naics_descriptions, '')), 'B') ||
  setweight(to_tsvector('english', COALESCE(owner_name, '')), 'C')
) STORED
```

Create GIN index: `CREATE INDEX businesses_fts_idx ON businesses USING GIN (fts);`
Create index on state: `CREATE INDEX businesses_state_idx ON businesses(state);`
Create index on business_type: `CREATE INDEX businesses_type_idx ON businesses(business_type);`
Create unique partial index on uei: `CREATE UNIQUE INDEX businesses_uei_idx ON businesses(uei) WHERE uei IS NOT NULL;`

**Freshness function:**
```sql
CREATE OR REPLACE FUNCTION freshness_status(verified_date DATE)
RETURNS TEXT AS $$
BEGIN
  IF verified_date >= CURRENT_DATE - INTERVAL '6 months' THEN
    RETURN 'fresh';
  ELSIF verified_date >= CURRENT_DATE - INTERVAL '1 year' THEN
    RETURN 'stale';
  ELSE
    RETURN 'outdated';
  END IF;
END;
$$ LANGUAGE plpgsql IMMUTABLE;
```

**Search RPC functions:**
```sql
CREATE OR REPLACE FUNCTION search_organizations(
  query_text TEXT DEFAULT NULL,
  filter_state TEXT DEFAULT NULL,
  filter_service_category TEXT DEFAULT NULL,
  filter_va_accredited BOOLEAN DEFAULT NULL,
  filter_confidence_grade TEXT DEFAULT NULL,
  page_number INT DEFAULT 1,
  page_size INT DEFAULT 20
)
RETURNS TABLE (
  id UUID, org_name TEXT, city TEXT, state TEXT, zip_code TEXT,
  services_offered TEXT, service_categories TEXT,
  confidence_grade TEXT, confidence_score NUMERIC,
  last_verified_date DATE, va_accredited BOOLEAN,
  phone TEXT, email TEXT, website TEXT,
  rank REAL, total_count BIGINT
) AS $$
DECLARE
  offset_val INT := (page_number - 1) * page_size;
BEGIN
  RETURN QUERY
  SELECT
    o.id, o.org_name, o.city, o.state, o.zip_code,
    o.services_offered, o.service_categories,
    o.confidence_grade, o.confidence_score,
    o.last_verified_date, o.va_accredited,
    o.phone, o.email, o.website,
    CASE
      WHEN query_text IS NOT NULL AND length(trim(query_text)) >= 3
      THEN ts_rank(o.fts, websearch_to_tsquery('english', query_text))
      ELSE 1.0::REAL
    END AS rank,
    COUNT(*) OVER() AS total_count
  FROM public.organizations o
  WHERE
    (query_text IS NULL OR length(trim(query_text)) < 3 OR o.fts @@ websearch_to_tsquery('english', query_text))
    AND (filter_state IS NULL OR o.state = filter_state)
    AND (filter_service_category IS NULL OR o.service_categories ILIKE '%' || filter_service_category || '%')
    AND (filter_va_accredited IS NULL OR o.va_accredited = filter_va_accredited)
    AND (filter_confidence_grade IS NULL OR o.confidence_grade = filter_confidence_grade)
  ORDER BY
    CASE
      WHEN query_text IS NOT NULL AND length(trim(query_text)) >= 3
      THEN ts_rank(o.fts, websearch_to_tsquery('english', query_text))
      ELSE 1.0::REAL
    END DESC,
    o.confidence_score DESC NULLS LAST
  LIMIT page_size OFFSET offset_val;
END;
$$ LANGUAGE plpgsql STABLE;

CREATE OR REPLACE FUNCTION search_businesses(
  query_text TEXT DEFAULT NULL,
  filter_state TEXT DEFAULT NULL,
  filter_business_type TEXT DEFAULT NULL,
  page_number INT DEFAULT 1,
  page_size INT DEFAULT 20
)
RETURNS TABLE (
  id UUID, legal_business_name TEXT, dba_name TEXT,
  city TEXT, state TEXT, zip_code TEXT,
  business_type TEXT, naics_descriptions TEXT,
  phone TEXT, email TEXT, website TEXT,
  owner_name TEXT, service_branch TEXT,
  last_verified_date DATE,
  rank REAL, total_count BIGINT
) AS $$
DECLARE
  offset_val INT := (page_number - 1) * page_size;
BEGIN
  RETURN QUERY
  SELECT
    b.id, b.legal_business_name, b.dba_name,
    b.city, b.state, b.zip_code,
    b.business_type, b.naics_descriptions,
    b.phone, b.email, b.website,
    b.owner_name, b.service_branch,
    b.last_verified_date,
    CASE
      WHEN query_text IS NOT NULL AND length(trim(query_text)) >= 3
      THEN ts_rank(b.fts, websearch_to_tsquery('english', query_text))
      ELSE 1.0::REAL
    END AS rank,
    COUNT(*) OVER() AS total_count
  FROM public.businesses b
  WHERE
    (query_text IS NULL OR length(trim(query_text)) < 3 OR b.fts @@ websearch_to_tsquery('english', query_text))
    AND (filter_state IS NULL OR b.state = filter_state)
    AND (filter_business_type IS NULL OR b.business_type = filter_business_type)
  ORDER BY
    CASE
      WHEN query_text IS NOT NULL AND length(trim(query_text)) >= 3
      THEN ts_rank(b.fts, websearch_to_tsquery('english', query_text))
      ELSE 1.0::REAL
    END DESC,
    b.legal_business_name ASC
  LIMIT page_size OFFSET offset_val;
END;
$$ LANGUAGE plpgsql STABLE;
```

**RLS policies (public read, admin write):**
```sql
ALTER TABLE public.organizations ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.businesses ENABLE ROW LEVEL SECURITY;

-- Everyone can read directory data (public resource)
CREATE POLICY "Directory organizations are publicly readable"
  ON public.organizations FOR SELECT USING (true);

CREATE POLICY "Directory businesses are publicly readable"
  ON public.businesses FOR SELECT USING (true);

-- Only admins can modify directory data
CREATE POLICY "Only admins can insert organizations"
  ON public.organizations FOR INSERT
  WITH CHECK (
    EXISTS (SELECT 1 FROM public.profiles WHERE id = auth.uid() AND role = 'admin')
  );

CREATE POLICY "Only admins can update organizations"
  ON public.organizations FOR UPDATE
  USING (
    EXISTS (SELECT 1 FROM public.profiles WHERE id = auth.uid() AND role = 'admin')
  );

CREATE POLICY "Only admins can insert businesses"
  ON public.businesses FOR INSERT
  WITH CHECK (
    EXISTS (SELECT 1 FROM public.profiles WHERE id = auth.uid() AND role = 'admin')
  );

CREATE POLICY "Only admins can update businesses"
  ON public.businesses FOR UPDATE
  USING (
    EXISTS (SELECT 1 FROM public.profiles WHERE id = auth.uid() AND role = 'admin')
  );
```

**TypeScript types in `src/lib/db/types.ts`:**
Create types that mirror the database columns for both tables:
- `Organization` type with all columns from organizations table
- `Business` type with all columns from businesses table
- `FreshnessStatus` = 'fresh' | 'stale' | 'outdated'
- `OrganizationSearchResult` type matching the search_organizations RPC return
- `BusinessSearchResult` type matching the search_businesses RPC return
- Helper function `getFreshnessStatus(lastVerifiedDate: string): FreshnessStatus` that mirrors the SQL function logic (< 6 months = fresh, < 1 year = stale, else outdated)
- Helper function `getFreshnessColor(status: FreshnessStatus): string` returning 'text-green-600' | 'text-yellow-600' | 'text-red-600'
- Helper function `getFreshnessLabel(status: FreshnessStatus): string` returning 'Verified' | 'Needs Review' | 'Outdated'
  </action>
  <verify>
Run `npm run build` to verify TypeScript types compile. Manually review migration SQL for syntax errors. Verify the migration can be applied to Supabase by running it in Supabase SQL editor (or `supabase db push` if CLI is configured).
  </verify>
  <done>
Migration file exists with both tables, GIN indexes, FTS generated columns, freshness function, search RPCs, and RLS policies. TypeScript types file exports Organization, Business, FreshnessStatus, and search result types with helper functions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build organization ETL pipeline with streaming CSV and Zod validation</name>
  <files>
    src/lib/etl/validators/organization.ts
    src/lib/etl/parsers/organization-import.ts
    scripts/import-organizations.ts
  </files>
  <action>
**Install dependencies:**
```bash
npm install csv-parse
```

**Zod validation schema (`src/lib/etl/validators/organization.ts`):**

Create a Zod schema that validates rows from the vet_org_directory CSV (50 columns). The CSV has these columns: org_name, org_name_alt, ein, org_type, street_address, street_address_2, city, state, zip_code, country, phone, email, website, ntee_code, ntee_description, irs_subsection, irs_filing_requirement, tax_exempt_status, ruling_date, mission_statement, services_offered, service_categories, eligibility_requirements, service_area, year_founded, fiscal_year_end, total_revenue, total_expenses, total_assets, total_liabilities, net_assets, annual_revenue_range, num_employees, num_volunteers, key_personnel, board_members, charity_navigator_rating, charity_navigator_score, cn_alert_level, va_accredited, accreditation_details, facebook_url, twitter_url, linkedin_url, instagram_url, youtube_url, data_sources, data_freshness_date, confidence_score, confidence_grade, confidence_detail, record_last_updated.

Key validation rules:
- `org_name` is REQUIRED (reject row if missing/empty)
- `ein` is optional but if present must match pattern `XX-XXXXXXX` (2 digits, dash, 7 digits)
- `state` is optional but if present must be 2 uppercase letters
- `phone` is optional, transform to cleaned format (strip non-digits, keep if 10+ digits)
- `website` is optional, basic URL format check (starts with http:// or https://)
- `email` is optional, basic email format check (contains @)
- `confidence_score` coerce to number, must be 0-1 range
- `confidence_grade` must be one of A, B, C, D, F
- `total_revenue`, `total_expenses`, `total_assets` coerce to number, nullable
- `va_accredited` transform "Yes"/"True"/"1" to true, everything else to false
- `data_sources` transform semicolon-separated string to string array
- `data_freshness_date` parse as date string (YYYY-MM-DD format), use as last_verified_date
- All other string fields: trim whitespace, convert empty strings to null

Export: `OrganizationSchema` (the Zod schema), `validateOrganizationRow` function that takes a raw CSV row object and returns `{ success: true, data: ValidatedOrg } | { success: false, error: string }`, and `ValidatedOrganization` type inferred from schema.

**Streaming CSV import (`src/lib/etl/parsers/organization-import.ts`):**

Create `importOrganizations` function that:
1. Takes `filePath: string` parameter (path to veteran_org_directory.csv)
2. Creates a read stream with csv-parse configured with `{ columns: true, skip_empty_lines: true, relax_column_count: true, bom: true }` (BOM handling is important -- the CSV has a BOM marker based on the header inspection)
3. For each row: validate with `validateOrganizationRow`
4. Accumulate valid records into a batch array
5. When batch reaches 1000 rows, insert into Supabase `organizations` table using service role client (bypass RLS)
6. Track and log validation errors (row number + error message)
7. After all rows processed, insert remaining batch
8. Return `{ imported: number, errors: { row: number, error: string }[], total: number }`

Use the SUPABASE_SERVICE_ROLE_KEY for the ETL client (bypass RLS for bulk import). Create a dedicated `createServiceClient()` helper in the import file using `createClient` from `@supabase/supabase-js` directly (not the SSR variant).

Log progress every 5000 rows: "Processed X/total rows (Y errors so far)"

**CLI script (`scripts/import-organizations.ts`):**

Create a TypeScript script that:
1. Accepts CSV file path as command-line argument (default: `../vet_org_directory/data/output/veteran_org_directory.csv` relative to project root)
2. Calls `importOrganizations(filePath)`
3. Logs summary: total rows, imported count, error count
4. Exits with code 1 if error rate > 10% (indicates data quality issue)

Add npm script to package.json: `"import:orgs": "npx tsx scripts/import-organizations.ts"`

**Important:** The CSV file has a BOM (byte-order mark) at the start. The csv-parse `bom: true` option handles this. Also, the first column name will be `org_name` (not `\ufefforg_name`) with BOM handling enabled.
  </action>
  <verify>
Run `npm run build` to verify all TypeScript compiles. Run `npm run import:orgs -- /Users/vincecain/Projects/vet_org_directory/data/output/veteran_org_directory.csv` to execute the import. Verify the import completes with < 10% error rate. Run a test query in Supabase SQL editor: `SELECT COUNT(*) FROM organizations;` should return 75K+ rows (allowing for some validation rejections from 85K total).
  </verify>
  <done>
Organization ETL pipeline imports 75K+ records from CSV into organizations table. Validation rejects invalid rows (missing org_name, malformed EIN) and logs errors. Import completes without manual intervention.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `SELECT COUNT(*) FROM organizations;` returns 75,000+ rows
2. `SELECT COUNT(*) FROM businesses;` returns 0 (businesses imported in Plan 02)
3. `SELECT * FROM organizations WHERE fts @@ to_tsquery('english', 'mental & health') LIMIT 5;` returns results with mental health organizations
4. `SELECT freshness_status(last_verified_date) FROM organizations LIMIT 1;` returns 'fresh', 'stale', or 'outdated'
5. `npm run build` succeeds with no TypeScript errors
6. TypeScript types in `src/lib/db/types.ts` export Organization, Business, FreshnessStatus
</verification>

<success_criteria>
- organizations and businesses tables exist with proper schema, indexes, and RLS
- 75K+ organization records imported from vet_org_directory CSV
- Full-text search works on organizations (tsvector + GIN index)
- Freshness function returns correct status based on last_verified_date
- Invalid rows rejected during import with logged errors
- TypeScript types match database schema
</success_criteria>

<output>
After completion, create `.planning/phases/02-resource-directory-data-pipeline/02-01-SUMMARY.md`
</output>
