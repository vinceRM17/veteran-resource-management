---
phase: 02-resource-directory-data-pipeline
plan: 02
type: execute
wave: 1
depends_on: ["02-01"]
files_modified:
  - src/lib/etl/validators/business.ts
  - src/lib/etl/parsers/business-import.ts
  - scripts/import-businesses.ts
autonomous: true

must_haves:
  truths:
    - "5,500+ business records are imported from SQLite into businesses table"
    - "Invalid business rows are rejected and logged during import"
    - "Each business has last_verified_date and business_type classification"
    - "Full-text search works on businesses table (tsvector + GIN index)"
  artifacts:
    - path: "src/lib/etl/validators/business.ts"
      provides: "Zod schema for validating business records from SQLite"
      exports: ["BusinessSchema", "validateBusinessRow"]
    - path: "src/lib/etl/parsers/business-import.ts"
      provides: "SQLite reader with batch insert for businesses"
      exports: ["importBusinesses"]
    - path: "scripts/import-businesses.ts"
      provides: "CLI entry point that runs the business import"
  key_links:
    - from: "src/lib/etl/parsers/business-import.ts"
      to: "src/lib/etl/validators/business.ts"
      via: "Zod validation before insert"
      pattern: "BusinessSchema\\.safeParse"
    - from: "src/lib/etl/parsers/business-import.ts"
      to: "supabase businesses table"
      via: "Supabase client batch insert"
      pattern: "supabase\\.from\\('businesses'\\)\\.insert"
---

<objective>
Build the ETL pipeline that imports 5,500+ veteran-owned businesses from the veteran-business-db SQLite database into the Supabase businesses table created in Plan 01.

Purpose: Populates the business directory so veterans can search for veteran-owned businesses by industry and location. This completes the data pipeline for both data sources (DATA-01 through DATA-04).

Output: Zod validation schema for businesses, SQLite-to-PostgreSQL import script, and 5,500+ business records in the database.
</objective>

<execution_context>
@/Users/vincecain/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vincecain/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-resource-directory-data-pipeline/02-RESEARCH.md
@.planning/phases/02-resource-directory-data-pipeline/02-01-SUMMARY.md
@supabase/migrations/00002_directory_schema.sql
@src/lib/db/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build business ETL pipeline with SQLite reader and Zod validation</name>
  <files>
    src/lib/etl/validators/business.ts
    src/lib/etl/parsers/business-import.ts
    scripts/import-businesses.ts
    package.json
  </files>
  <action>
**Install dependencies:**
```bash
npm install better-sqlite3
npm install -D @types/better-sqlite3
```

Use `better-sqlite3` (synchronous SQLite reader for Node.js) instead of the Python sqlite3 module. This is the standard Node.js library for reading SQLite files directly.

**Zod validation schema (`src/lib/etl/validators/business.ts`):**

Create a Zod schema that validates rows from the veteran-business-db SQLite `businesses` table. The table has these columns: id, uei, cage_code, legal_business_name, dba_name, physical_address_line1, physical_address_line2, city, state, zip_code, country, phone, email, website, business_type, naics_codes, naics_descriptions, service_branch, certification_date, registration_status, registration_expiration, entity_start_date, source, latitude, longitude, distance_miles, date_added, date_updated, notes, owner_name, linkedin_url, yelp_rating, yelp_review_count, yelp_url.

Key validation rules:
- `legal_business_name` is REQUIRED (reject row if missing/empty)
- `uei` is optional but if present must be a non-empty string
- `state` is optional but if present must be 2 uppercase letters
- `phone` is optional, trim whitespace
- `website` is optional, basic URL check
- `email` is optional, basic email check (contains @)
- `business_type` is optional, default to 'Veteran Owned Small Business' if missing
- `latitude` and `longitude` coerce to number, nullable
- `naics_codes` and `naics_descriptions` are optional strings
- `date_added` or `date_updated` used as last_verified_date (take the more recent one, format as YYYY-MM-DD)
- All other string fields: trim whitespace, convert empty strings to null

Export: `BusinessSchema`, `validateBusinessRow`, and `ValidatedBusiness` type.

**SQLite import (`src/lib/etl/parsers/business-import.ts`):**

Create `importBusinesses` function that:
1. Takes `dbPath: string` parameter (path to veteran_businesses.db)
2. Opens SQLite database using better-sqlite3 in read-only mode
3. Queries `SELECT * FROM businesses` (5,567 rows -- small enough to read all at once)
4. For each row: validate with `validateBusinessRow`
5. Accumulate valid records into batch array
6. Insert into Supabase `businesses` table in batches of 500 (smaller dataset, smaller batches)
7. Track and log validation errors
8. Close SQLite connection
9. Return `{ imported: number, errors: { row: number, error: string }[], total: number }`

Use the same `createServiceClient()` pattern from the organization import (SUPABASE_SERVICE_ROLE_KEY to bypass RLS).

Map SQLite columns to PostgreSQL columns:
- `id` (SQLite integer) is NOT mapped -- use Supabase-generated UUID
- `date_added` / `date_updated` map to `last_verified_date` (take the most recent)
- `distance_miles` is NOT mapped (relative to Active Heroes location, not useful in general)
- `yelp_rating`, `yelp_review_count`, `yelp_url`, `linkedin_url` are NOT mapped (not in PostgreSQL schema, not needed for MVP)
- `certification_date` is NOT mapped (not in PostgreSQL schema)
- All other fields map by name

**CLI script (`scripts/import-businesses.ts`):**

Create a TypeScript script that:
1. Accepts SQLite database path as command-line argument (default: `../veteran-business-db/veteran_businesses.db` relative to project root)
2. Calls `importBusinesses(dbPath)`
3. Logs summary: total rows, imported count, error count
4. Exits with code 1 if error rate > 10%

Add npm script to package.json: `"import:businesses": "npx tsx scripts/import-businesses.ts"`

Also add a combined import script: `"import:all": "npm run import:orgs && npm run import:businesses"` for convenience.
  </action>
  <verify>
Run `npm run build` to verify TypeScript compiles. Run `npm run import:businesses -- /Users/vincecain/Projects/veteran-business-db/veteran_businesses.db` to execute the import. Verify the import completes with < 10% error rate. Run test query: `SELECT COUNT(*) FROM businesses;` should return 5,000+ rows.
  </verify>
  <done>
Business ETL pipeline imports 5,000+ records from SQLite into businesses table. Validation rejects invalid rows and logs errors. Import completes without manual intervention. Both `import:orgs` and `import:businesses` npm scripts work.
  </done>
</task>

</tasks>

<verification>
After task completes:
1. `SELECT COUNT(*) FROM businesses;` returns 5,000+ rows
2. `SELECT * FROM businesses WHERE fts @@ to_tsquery('english', 'construction') LIMIT 5;` returns results
3. `SELECT DISTINCT business_type FROM businesses;` shows veteran business type classifications
4. `npm run build` succeeds
5. `npm run import:all` runs both imports without errors
</verification>

<success_criteria>
- 5,000+ business records imported from veteran-business-db SQLite
- Full-text search works on businesses table
- Invalid rows rejected with logged errors
- Combined import script (`import:all`) runs both ETL pipelines
</success_criteria>

<output>
After completion, create `.planning/phases/02-resource-directory-data-pipeline/02-02-SUMMARY.md`
</output>
